{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "preprocess_data.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5D6TsQlvSZH"
      },
      "source": [
        "# [Data Preprocessing](http://colab.research.google.com/github/boringPpl/presidential_debates_comments_clustering/blob/main/preprocess_data.ipynb)\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "### 1.1 Hardware\n",
        "\n",
        "First, check that the runtime in Google Colab is set to GPU. If it is not, go to **Runtime > Change runtime type** and change the **Hardware Accelerator** to **GPU**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsSzCZyxy9As",
        "outputId": "e279e1ea-4bf2-4030-d243-5e487537d078"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 22 13:01:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    28W /  70W |  14388MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FT94kk61QKd"
      },
      "source": [
        "### 1.2 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49kLid8avSZJ",
        "outputId": "c4ab065f-313f-4a80-c641-2f9640d112ae"
      },
      "source": [
        "!git clone https://github.com/boringPpl/presidential_debates_comments_clustering.git\n",
        "%cd presidential_debates_comments_clustering\n",
        "!pip install -qr requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'presidential_debates_comments_clustering'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 94 (delta 39), reused 54 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (94/94), done.\n",
            "/content/presidential_debates_comments_clustering\n",
            "\u001b[K     |████████████████████████████████| 133kB 16.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.4MB 33.5MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 53.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n",
            "\u001b[?25h  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZevAggEvSZK",
        "outputId": "c38d7913-61b4-4215-fb89-e20f9588a683"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import emoji\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embedder = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:15<00:00, 25.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGSugYCGvSZK"
      },
      "source": [
        "## 2. Exploratory Data Analysis\n",
        "\n",
        "### 2.1 Load data into Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWRl3_GzvSZK"
      },
      "source": [
        "def load_comments(video_id):\n",
        "    filename = f'data/{video_id}_csv_final.csv'\n",
        "    df = pd.read_csv(filename, index_col=0)\n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLffUHsTvSZL"
      },
      "source": [
        "df1 = load_comments('bPiofmZGb8o')\n",
        "comments1 = df1['Comments']\n",
        "df1['Updated At'] = pd.to_datetime(df1['Updated At'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNpUIfUnvSZL"
      },
      "source": [
        "### 2.2 Exploratory data analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "bnHLsvy7vSZL",
        "outputId": "c324b7a9-e448-486c-d46a-8ca8b8a93872"
      },
      "source": [
        "print(f'The shape of the dataframe is: {df1.shape}')\n",
        "print(f\"The time range for the data is: {df1['Updated At'].min():%Y-%m-%d %H%Mh} to {df1['Updated At'].max():%Y-%m-%d %H%Mh}\")\n",
        "df1.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataframe is: (49608, 6)\n",
            "The time range for the data is: 2020-10-23 0243h to 2021-01-18 0945h\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comments</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>Reply Count</th>\n",
              "      <th>Like Count</th>\n",
              "      <th>Updated At</th>\n",
              "      <th>Viewer Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25:28\\nTrump: Did you hear anything?\\nBiden: N...</td>\n",
              "      <td>UgxJhAbG8Z-yTkfAPPl4AaABAg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-18 09:45:30</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53:39 trump the idiot.\\nYour the big man! I do...</td>\n",
              "      <td>UgyHNoBZEukz5n3FHgR4AaABAg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-18 03:57:03</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Trump is the President of the USA !  no biden...</td>\n",
              "      <td>UgzhlTI8JkT8n_sKDuR4AaABAg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-18 03:36:51</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Driving home from school today because that We...</td>\n",
              "      <td>UgxJmfHfPpwcEdkLFNV4AaABAg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-18 02:03:25</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump won election the swamp rats in Washingto...</td>\n",
              "      <td>UgwZT9ggKbgHu6KbOgN4AaABAg</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-18 00:44:14</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Comments  ... Viewer Rating\n",
              "0  25:28\\nTrump: Did you hear anything?\\nBiden: N...  ...          none\n",
              "1  53:39 trump the idiot.\\nYour the big man! I do...  ...          none\n",
              "2   Trump is the President of the USA !  no biden...  ...          none\n",
              "3  Driving home from school today because that We...  ...          none\n",
              "4  Trump won election the swamp rats in Washingto...  ...          none\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "2lhHs12yvSZL",
        "outputId": "fde13103-3356-4344-c08f-32706bc95c61"
      },
      "source": [
        "ax = df1['Updated At'].hist(bins=25, figsize=(12, 5))\n",
        "ax.set_yscale('log')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZyIA-tfvSZM",
        "outputId": "27728245-61b3-45a8-89e5-c1ccd256353d"
      },
      "source": [
        "import emoji\n",
        "print(emoji.demojize('trending 😉\t'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trending :winking_face:\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lej-nAeXvSZM"
      },
      "source": [
        "df_corpus = df1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWq6agYPvSZM"
      },
      "source": [
        "df_corpus.rename(columns={'Comments': 'comment_text'}, inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvynIneBvSZN"
      },
      "source": [
        "# De-emojize\n",
        "df_corpus['comments_cleaned'] = df_corpus['comment_text'].apply(emoji.demojize)\n",
        "\n",
        "# Replace the colons, and \\n with a space\n",
        "df_corpus['comments_cleaned'] = df_corpus['comments_cleaned'].str.replace('[\\n:]', ' ', regex=True)\n",
        "df_corpus['comments_cleaned'] = df_corpus['comments_cleaned'].str.replace(r'\\\\n', ' ', regex=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iwHuvFPvSZN"
      },
      "source": [
        "df_corpus['comments_cleaned'] = df_corpus['comments_cleaned'].str.lower()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YxEPBoTvSZN",
        "outputId": "b0661969-46f5-46c6-b8fa-a92520f30bfb"
      },
      "source": [
        "df_corpus.drop_duplicates(subset=['comments_cleaned'], inplace = True)\n",
        "df_corpus.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47184, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEWnr931vSZO"
      },
      "source": [
        "# remove special characters\n",
        "df_corpus['comments_cleaned'] = df_corpus['comments_cleaned'].str.replace('[^a-zA-Z0-9]', ' ')\n",
        "\n",
        "# remove white spaces\n",
        "df_corpus['comments_cleaned'] = df_corpus['comments_cleaned'].str.replace('\\s+', ' ', regex=True)\n",
        "df_corpus['comments_cleaned'] = df_corpus['comments_cleaned'].str.strip()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1DNb4RIvSZO"
      },
      "source": [
        "df_corpus['comments_cleaned'].to_csv('meta.tsv', columns=['comments_cleaned'], index= False, header= False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX6NhMd5vSZO"
      },
      "source": [
        "The longest comment has 2993 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XEek6ClvSZP",
        "outputId": "5a1acf7f-25ee-4616-e9fd-dfc4173f00a9"
      },
      "source": [
        "df_corpus['comments_cleaned'].apply(len).sort_values(ascending=False).head(20)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13649    9649\n",
              "8742     9572\n",
              "14669    9357\n",
              "39329    8032\n",
              "18645    7502\n",
              "13222    7429\n",
              "4739     7033\n",
              "48888    6823\n",
              "17436    6714\n",
              "26363    6246\n",
              "30068    6076\n",
              "19148    5943\n",
              "9159     5886\n",
              "46668    5758\n",
              "4851     5732\n",
              "15616    5653\n",
              "5239     5653\n",
              "3757     5540\n",
              "29311    5498\n",
              "13054    5313\n",
              "Name: comments_cleaned, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcv7vIhgvSZP"
      },
      "source": [
        "## Sentence Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZboLRN5vSZP"
      },
      "source": [
        "corpus_embeddings = embedder.encode(df_corpus[\"comments_cleaned\"].values.tolist())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UcZAIVdvSZP",
        "outputId": "d2085050-a6a0-4434-c5ef-a8a00ec9c098"
      },
      "source": [
        "corpus_embeddings = np.array(corpus_embeddings)\n",
        "corpus_embeddings.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47184, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfBQw8jWvSZP"
      },
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, corpus_embeddings, 90)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDMPsKE5vSZQ"
      },
      "source": [
        "## Google Universal Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd-JU_exvSZQ",
        "outputId": "90bbf237-7c92-4c3a-cdda-7f1e36f84765"
      },
      "source": [
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5, Total size: 577.10MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "img28FjgvSZQ",
        "outputId": "ec48c02d-a26e-4011-8c13-7e528ba1098c"
      },
      "source": [
        "#@title Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the emtbedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "message_embeddings = embed(messages)\n",
        "\n",
        "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "  print(\"Message: {}\".format(messages[i]))\n",
        "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "  message_embedding_snippet = \", \".join(\n",
        "      (str(x) for x in message_embedding[:3]))\n",
        "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message: Elephant\n",
            "Embedding size: 512\n",
            "Embedding: [-0.0345856137573719, -0.017990121617913246, 0.0019805720075964928, ...]\n",
            "\n",
            "Message: I am a sentence for which I would like to get its embedding.\n",
            "Embedding size: 512\n",
            "Embedding: [0.05833390727639198, -0.0818500965833664, 0.06890934705734253, ...]\n",
            "\n",
            "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the emtbedding will be.\n",
            "Embedding size: 512\n",
            "Embedding: [-0.018883462995290756, -0.0031109179835766554, -0.03367338702082634, ...]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFNDvKCSvSZQ"
      },
      "source": [
        "## Semantic Textual Similarity Task Example\n",
        "\n",
        "The embeddings produced by the Universal Sentence Encoder are approximately normalized. The semantic similarity of two sentences can be trivially computed as the inner product of the encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyhhupo_vSZR"
      },
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, message_embeddings_, 90)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKYBmWgwvSZR"
      },
      "source": [
        "messages0 = df_corpus[\"comments_cleaned\"].values.tolist()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBn-kGVuvSZR",
        "outputId": "1ee918aa-70f6-42ae-f545-e97915320773"
      },
      "source": [
        "df_corpus[\"comments_cleaned\"].apply(len).max()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9649"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF4xETc3vSZS"
      },
      "source": [
        "# embeddings_long = embed([df_corpus['comments_cleaned'].loc[617593]])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOe8pabYvSZS"
      },
      "source": [
        "# [USComments.csv] Take only the first 2200 characters of each comment. Crashes at 2300+\n",
        "# [Presidential Debate 1] < 2200. Trying 2000 [Nope]. Trying 1800 [Nope].\n",
        "# Trying 1000 [Nope]. Trying 500 [Nope]. Trying 200.\n",
        "max_chars = 200\n",
        "messages = df_corpus[\"comments_cleaned\"].apply(\n",
        "    lambda x: x[:max_chars]\n",
        ").values.tolist()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "V_QCoFe2vSZS",
        "outputId": "67d6aebc-8e3a-4d35-d2d7-29eb325eaf1f"
      },
      "source": [
        "embeddings = embed(messages)\n",
        "np.savetxt('vecs.tsv', embeddings , delimiter=\"\\t\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ee557c7d4c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vecs.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-fa9d65602c5b>\u001b[0m in \u001b[0;36membed\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"module %s loaded\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodule_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[47184,82,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderTransformer/Transformer/SparseTransformerEncode/Layer_0/SelfAttention/SparseMultiheadAttention/ComputeQKV/ScatterNd}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/EncoderTransformer/Transformer/layer_prepostprocess/layer_norm/add_1/_246]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[47184,82,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderTransformer/Transformer/SparseTransformerEncode/Layer_0/SelfAttention/SparseMultiheadAttention/ComputeQKV/ScatterNd}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_restored_function_body_82308]\n\nFunction call stack:\nrestored_function_body -> restored_function_body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlBGndw4vSZV"
      },
      "source": [
        "pd.Series(messages).apply(len).max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH6VTjAVvSZV"
      },
      "source": [
        "### PCA reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gso-q5SPvSZV"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8e9cd7JvSZV"
      },
      "source": [
        "#scale the data 0-1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "rescaled = scaler.fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLfDLcITvSZV"
      },
      "source": [
        "pca = PCA().fit(rescaled)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4qYCfpbvSZV"
      },
      "source": [
        "# need around 200 components to describe 100% of variance\n",
        "pca = PCA(n_components = 50)\n",
        "reduced_embeds = pca.fit_transform(rescaled)\n",
        "print(\"Original shape:   \", rescaled.shape)\n",
        "print(\"Transformed shape:\", reduced_embeds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMYogewKvSZW"
      },
      "source": [
        "# pca = PCA(n_components = 50)\n",
        "# reduced_embeds = pca.fit_transform(rescaled)\n",
        "# reduced_embeds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwHbLUzpvSZW"
      },
      "source": [
        "## Run HDBScan to find the clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVxDNTW4vSZX"
      },
      "source": [
        "import hdbscan\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN()\n",
        "clusterer.fit(reduced_embeds)\n",
        "clusterer.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdzdxO4gvSZX"
      },
      "source": [
        "clusterer.labels_.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuyjHVj7vSZX"
      },
      "source": [
        "df_corpus['hdb_labels'] = clusterer.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItoZ2c79vSZX"
      },
      "source": [
        "df_corpus.to_csv('meta_lab.tsv', columns=['comments_cleaned','hdb_labels'], index= False, header= True, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGK0xEcJvSZY"
      },
      "source": [
        "!head -5 meta.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVw6ZMisvSZY"
      },
      "source": [
        "!head -5 meta_lab.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh1NbTfbvSZY"
      },
      "source": [
        "Save`vecs.tsv` and `meta_lab.tsv` and load them into http://projector.tensorflow.org/ (use `vecs.tsv` for Step 1 and `meta_lab.tsv` for step 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HLvTekSvSZY"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('vecs.tsv')\n",
        "files.download('meta_lab.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UdLpr_cvaia"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}